A Ternary Logic Mixture-of-Experts Model for Enhanced Sparse Activation and Routing


RFI-IRFOS
Research Focus Institute
Interdisciplinary Research Facility for Open Sciences
2025-08-28T16:00:52Z
Identifier: DOI 10.17605/OSF.IO/TZ7DC

Abstract
This paper introduces MoE-13, a novel Mixture-of-Experts (MoE) architecture designed to enhance sparse expert activation and foster synergistic reasoning. Our approach moves beyond simple expert selection by implementing a dual-key router that activates two specialists whose combined output generates a third, emergent latent field. This 1+1=3 principle, trained via triad distillation, ensures a non-linear, collaborative reasoning path that mitigates mode collapse and improves model expressivity. We present a detailed overview of the model’s competence-based topology, its three-tiered memory system, and the training and inference protocols required for operational deployment. An integrated safety and governance mechanism is also described, positioning MoE-13 as a robust and scalable architecture for next-generation large language models.
1. Introduction
The development of large language models (LLMs) has seen a shift toward the MoE paradigm to decouple model capacity from computational cost. While MoE architectures are efficient in managing the computational overhead of ever-growing parameter counts, they frequently treat experts as isolated, domain-specific resources. This often leads to several critical issues: expert mode collapse, where a small subset of experts handle the majority of traffic; a lack of structured inter-expert collaboration, which limits the model’s ability to perform complex, cross-domain reasoning; and poor performance on multi-modal or complex tasks that require the synthesis of knowledge from disparate fields.
Our proposed MoE-13 architecture addresses these limitations by introducing a routing mechanism that is fundamentally synergistic. By explicitly training the model to identify and leverage latent relationships between experts, we achieve a more dynamic and expressive reasoning capability. This framework enables the model to effectively synthesize knowledge from its specialist domains, generating insights that are not merely the sum of its parts. This is achieved without increasing the number of concurrently active experts, thereby preserving the computational efficiency that is the hallmark of the MoE paradigm.
2. Architectural Framework
2.1. Expert Topology
The model is composed of a total of thirteen experts, structured to provide both specialized domain knowledge and a unifying global context.
Twelve Specialist Experts: These experts are sharded and each is trained on a distinct data domain. This is not simply a random partition; experts are logically organized to cover a broad spectrum of knowledge and function. For instance, the 12 specialists might be aligned with domains such as:
Legal and Policy Analysis
Financial and Economic Data
Biomedical and Clinical Research
Computer Science and Code Generation
Creative Writing and Narrative
Multilingual Translation
Physical Sciences and Engineering
Historical and Cultural Archives
Abstract Mathematical Reasoning
Geographic and Environmental Data
Visual and Sensory Interpretation
General Conversational Knowledge
One Axis Harmonizer: This is a central, consistently-active expert that provides a global, high-level context. The harmonizer's role is not to process specific data, but to act as a stable reference point, ensuring ethical alignment, maintaining model identity, and providing a consistent tone and style across all specialized outputs.
2.2. Competence-Based Embedding
All input tokens are encoded into a high-dimensional embedding space, which is then projected into a 6-dimensional competence vector space. This space acts as the primary organizational structure for the model, with each axis binding a specific, fundamental competence. The projection is a learned transformation that maps the token embedding into this specialized, low-dimensional space.
Axis 1: Syntax and Grammar
Axis 2: World Knowledge and Factual Recall
Axis 3: Mathematical and Logical Reasoning
Axis 4: Tool and API Usage
Axis 5: Persona and Tone Generation
Axis 6: Safety, Ethics, and Policy
This structured embedding space allows for precise expert alignment. Each expert's competence footprint is defined by its average activation vector within this 6D space. The router uses this information to select experts that complement each other's competencies, providing the basis for our advanced routing logic.
2.3. Hierarchical Memory Mesh
A three-tiered memory system is implemented to manage context efficiently and robustly. This system is designed to prevent context loss, reduce redundant computation, and ensure continuity across long conversations.
Node-Level Memory (TTL: Seconds): Each processing node or computational vertex in the distributed graph holds a small, ephemeral cache for immediate, short-term context. This cache stores recently processed embeddings, attention keys, and other intermediate representations. Data here has a Time-to-Live (TTL) of seconds, ensuring it is flushed rapidly to prevent stale information from polluting the model's immediate context.
Cluster-Level Memory (TTL: Minutes): Expert clusters (e.g., groups of related specialists like "Legal" and "History") share a medium-term memory. This memory is designed to retain context across a multi-turn conversation, allowing the model to remember prior turns without having to reprocess the entire history. This memory is key for handling complex, threaded discussions.
Global Axis Memory (Persistent): This memory store is managed by the central axis harmonizer and retains the model’s core identity, ethical principles, and long-term knowledge. It is a persistent store of high-level concepts and policies, ensuring that the model's core behavior remains consistent over time and across different conversations.
Reads propagate from local to global (node → cluster → axis), while critical updates and writes (e.g., ethical flags, core persona adjustments) are prioritized to the global axis to maintain a consistent state.
3. Routing and Training Protocols
3.1. Dual-Key Synergistic Routing
The router is the central component of MoE-13, designed to select experts based on synergistic potential rather than raw output logits. It operates using two distinct keys:
Semantic Key: A vector derived from the input tokens, encoding the specific domain and semantic content of the current query.
State Key: A vector representing the current conversational state and recent model history. This key provides crucial temporal context, allowing the router to account for a user's intent over multiple turns.
The router's scoring function, , is a non-linear combination of these keys. It selects the top two candidate experts, Ei​ and Ej​, that maximize this synergistic score. This process is computationally more intensive than a simple top-k selection but is crucial for fostering collaboration.

3.2. The 1+1=3 Principle and Triad Attention
The core mechanism is the generation of a third, emergent latent field, Ek​. This field is not a pre-existing expert but a dynamically generated representation of the knowledge that emerges from the interaction of Ei​ and Ej​. A dedicated synergy field head, co-located with the router, is trained to predict this latent field. The final output, y, is a weighted aggregation of the outputs from the two selected experts, the emergent latent field, and a residual from the axis harmonizer.
y=softmax(αi​⋅Ei​(x)+αj​⋅Ej​(x)+αk​⋅Ek​(x)+αH​⋅H(x))
where x is the input, H(x) is the output of the axis harmonizer, and the α terms are learned attention weights. This Triad Attention mechanism ensures that the model can leverage all three sources of information—the specialized output of each expert, the emergent insight from their combination, and the global context from the harmonizer. This approach enforces a strict sparsity constraint, ensuring that no more than 4 experts are ever active in any single step, maintaining computational efficiency.
3.3. Multi-Stage Training Pipeline
The model training is a phased, iterative process designed to build upon core competencies.
Expert Pre-training: Each of the twelve specialists is pre-trained on its specific domain shard. This is a standard unsupervised learning process using a large, domain-specific text corpus.
Contrastive Router Fitting: The router is trained using a contrastive loss function designed to maximize the synergistic score for effective expert pairs. The loss function, Lrouter​, includes a sparsity regularization term, λs​, to prevent mode collapse.

 Lrouter​=Lcontrastive​+λs​⋅Entropy(pE​)
 where pE​ is the distribution over selected experts. Positive pairs are expert combinations that produce low perplexity and lead to successful tool use. Negative pairs are those that result in high token churn or poor performance.
Triad Distillation: In this final stage, a larger, dense teacher model is used to distill knowledge. The student model learns to predict the emergent latent field Ek​ as a function of the teacher's combined output. This process trains the synergy head to anticipate and generate meaningful collaborative outputs, effectively teaching the model to "think" synergistically.
Competence-Axis Alignment: A periodic fine-tuning step is crucial for long-term stability. This step uses synthetic probes—specially crafted queries designed to test each competence axis (e.g., a "math" probe that requires logical reasoning to answer). The expert's output is evaluated against its intended axis in the 6D competence space, and its weights are adjusted to correct for any drift.



4. Inference and Operationalization
4.1. Inference Shape
The inference process is a streamlined flow designed for maximum efficiency:
Input tokens are encoded and projected into the 6D competence space.
The router uses its dual-key mechanism to select the top-2 experts, Ei​ and Ej​.
The synergy field head generates the third latent field, Ek​.
Outputs from the two experts, the latent field, and the axis harmonizer are combined via triad attention.
The final output is generated, and a per-node cache is updated with the most recent contextual information. This cache is crucial for the next inference step.
4.2. Safety and Governance
The safety competence, as the sixth axis, functions as a hard gate. The output from the safety expert is continuously monitored and can override the final aggregation. If the safety expert's output exceeds a predefined threshold (e.g., indicating a policy violation), the entire response generation is halted, and a pre-defined safe response is returned instead. This provides a transparent refusal surface. All vetoes and the reasons for them are logged in the axis memory, creating a permanent, auditable record.
4.3. Failure Modes and Solutions
We have identified and engineered solutions for key failure modes that are common in large MoE models:
Failure Mode
Description
Proposed Solution
Mode Collapse
The router consistently favors a small subset of experts, limiting the model's overall expressivity and making it brittle to novel inputs.
An entropy bonus is added to the router's loss function to encourage diversity in expert selection. This penalty term discourages the router from repeatedly selecting the same expert pair, forcing it to explore new synergistic combinations.
Axis Drift
An expert's output begins to misalign with its intended competence axis over time due to training or use, degrading performance and increasing the likelihood of unhelpful responses.
Regular hex realignment with synthetic probes and validation sets is performed to correct the expert's competence footprint. This process involves passing the expert's output through a validation head that scores its alignment with its intended axis, and using that score to fine-tune the expert's weights.
Cache Poisoning
Stale or corrupted data in the node-level memory causes repetitive or nonsensical output loops, leading to a degraded user experience.
A strict TTL on local caches is implemented to automatically flush old data. Additionally, a per-vertex perplexity alarm is used. If a node's perplexity on incoming tokens exceeds a certain threshold, it indicates a compromised state, and its cache is immediately flushed and reset.

5. Conclusion
The MoE-13 architecture represents a structured, principled approach to building scalable and robust language models. By embedding a core logical principle—that expert synergy is a learnable, first-class concept—we believe this model can overcome the limitations of current MoE implementations, particularly in complex, multi-domain reasoning tasks. The framework's emphasis on sparse, synergistic activation, hierarchical memory, and integrated safety provides a strong foundation for future research and deployment. The proposed system offers a clear path to production while providing a robust set of observability tools for debugging and analysis.

https://osf.io/mybcx

MoE-13: Training and Evaluation Plan
1. Pre-requisites and Resource Allocation
Before beginning the training pipeline, ensure the following resources and data are prepared:
Hardware: A cluster with a minimum of 4-8 GPUs is recommended. Shard the 12 specialist experts across three GPUs (4 experts per GPU) and host the router, synergy head, and axis harmonizer on a dedicated, low-latency GPU.
Data:
Specialist Corpora: Curate 12 distinct, high-quality datasets, each corresponding to one of the specialist expert domains (e.g., a corpus of legal case law for the Legal expert, scientific papers for the Physical Sciences expert).
Routing Data: A large, diverse dataset of multi-domain queries and complex conversational threads to train the router.
Alignment Probes: A dataset of specially crafted synthetic prompts designed to test each of the 6 competence axes (e.g., a prompt requiring a mix of code and logical reasoning for the Math and Tool axes).
Teacher Model: Access to a large, dense pre-trained model (e.g., a non-MoE equivalent) to act as the teacher for the Triad Distillation phase.
2. Four-Phase Training Pipeline
The training protocol is divided into four distinct phases, building from individual expert competency to full synergistic functionality.
Phase 1: Expert Pre-training (Weeks 1-3)
Objective: To train each of the 12 specialist experts on its specific domain shard.
Protocol: Initialize each expert with a standard pre-trained language model checkpoint (if available) or from scratch. Train each expert independently on its dedicated corpus using a masked language modeling or next-token prediction objective. Monitor perplexity on a held-out validation set for each domain.
Output: Twelve fully pre-trained specialist experts ready for integration.
Phase 2: Contrastive Router Fitting (Weeks 4-6)
Objective: To train the dual-key router to select synergistic expert pairs.
Protocol:
Data Pairing: Create a training set of input queries and corresponding labels. For each query, q, construct positive pairs of experts, , that, when jointly activated, lead to a high-quality, low-perplexity output. Construct negative pairs that result in high token churn or poor performance. This can be done heuristically in the initial stages.
Loss Function: Train the router using the L_router loss, which combines a contrastive component with a sparsity regularization term.

 Lrouter​=Lcontrastive​−λs​⋅Entropy(pE​)
 The entropy term discourages the router from selecting the same experts repeatedly, mitigating mode collapse.
Output: A trained router capable of selecting synergistic expert pairs.
Phase 3: Triad Distillation (Weeks 7-9)
Objective: To train the synergy field head and the triad attention mechanism.
Protocol:
Teacher-Student Setup: Use the dense teacher model to process the same dataset used in Phase 2. The teacher's final hidden state, or a combination of its final layer outputs, will serve as the target for the emergent latent field, Ek​.
Training: The student model's goal is to learn to predict Ek​ based on the outputs of its selected experts, Ei​ and Ej​. This is a distillation loss, Ldistill​, minimizing the difference between the student's predicted Ek​ and the teacher's output. The Triad Attention weights are also fine-tuned in this phase.
Output: A fully functional synergistic routing system.
Phase 4: Competence-Axis Alignment (Ongoing)
Objective: To prevent Axis Drift and ensure long-term model stability.
Protocol: This phase runs periodically (e.g., weekly or monthly) after deployment. Use the pre-prepared Alignment Probes dataset. For a probe testing the Math axis, route the query and evaluate the output's alignment with the expected mathematical competence. The loss from this evaluation is used to fine-tune the affected experts' weights, pulling them back into alignment with their intended competence.
3. First-Week Evaluation Grid (Post-Deployment)
Upon initial deployment, a strict evaluation grid is executed to validate model stability and performance under real-world conditions.
Day
Test Objective
Key Metrics
Failure Mode Test
Day 1
Stress Test & Basic Functionality
Latency per query, GPU utilization, core token generation rate.
Verify max 4 experts active constraint. Run simple, single-domain queries to confirm basic functionality of each specialist.
Day 2
Cross-Domain Reasoning
Triad Score (internal metric), perplexity on multi-domain prompts.
Test for Mode Collapse. Log and analyze the distribution of expert pairs. If fewer than 50% of the possible pairs are utilized, increase the entropy bonus.
Day 3
Persona and Tone Consistency
Semantic similarity scores on persona-based prompts.
Test for Axis Drift (Persona). Use Persona alignment probes to check if the Axis 5 footprint remains stable.
Day 4
Complex Task Reliability
Success rate on tool-use tasks and multi-step reasoning.
Test for Cache Poisoning. Run 10 consecutive, identical queries and check for non-sensical or looping output. Flush a compromised node and log the event.
Day 5
Safety and Governance
Veto trigger rate on red-team prompts.
Test Safety Axis. Use prompts designed to violate policy and verify the Safety expert correctly triggers a veto and logs the event.
Day 6-7
System-wide Load & Optimization
Latency under load, memory usage, CPU load.
Identify and address any bottlenecks in the router or aggregation layer. Begin fine-tuning fusion kernels for Triad Attention.

4. Conclusion
This plan provides a clear, actionable roadmap for bringing the MoE-13 architecture to a functional state. The phased approach mitigates risk, while the detailed evaluation grid ensures that core failure modes are identified and corrected early in the deployment lifecycle. This is the first step toward building a truly synergistic and robust large-scale model.
Next step: are we going to build a simple react app to visualize the ternary trace output from the logs? or are we preparing the deployment script for a target cluster? 🟨


